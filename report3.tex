\documentclass[12pt]{jsarticle}
\usepackage[dvipdfm,left=1.5cm,right=1.5cm,top=2cm]{geometry}
\usepackage[dvipdfmx]{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{bm}
\usepackage{comment}
\usepackage{framed}
\usepackage{tabularx}
\usepackage{float}

\graphicspath{{image/}{textfile/}{AE/}}

\setlength{\topmargin}{-1in}
\addtolength{\topmargin}{5mm}
\setlength{\headheight}{5mm}
\setlength{\headsep}{0mm}
\setlength{\textheight}{\paperheight}
\addtolength{\textheight}{-25mm}
\setlength{\footskip}{5mm}

\newcommand{\frontpage}[3]{
\begin{center}
　\\
\vspace{15em}{\LARGE{}レポート課題}\\
　\\
{\Huge\bf#1}\\
\vspace{30em}
{\LARGE\today}\\
\vspace{2em}
{\LARGE#2　#3}
\end{center}
\thispagestyle{empty}
\clearpage
\setcounter{page}{1}
}

\newcommand{\result}[5]{
\begin{minipage}{0.05\hsize}
(#1)
\end{minipage}
\begin{minipage}{0.22\hsize}
\includegraphics[width=\linewidth]{#2}
\end{minipage}
\begin{minipage}{0.22\hsize}
\includegraphics[width=\linewidth]{#3}
\end{minipage}
\begin{minipage}{0.22\hsize}
\includegraphics[width=\linewidth]{#4}
\end{minipage}
\begin{minipage}{0.22\hsize}
\includegraphics[width=\linewidth]{#5}
\end{minipage}
\\
}

\begin{document}

\frontpage
{自己符号化器の特性評価}
{S152114}
{宮地雄也}

\section{実験目的}

ニューラルネットワークによる生成モデルの基礎となる自己符号化器の原理および特性について理解する．

\section{実験原理}

\subsection{自己符号化器（Autoencoder，AE）}
自己符号化器とは入力に対して同じものを出力するように学習するニューラルネットワークである．
教師なし学習に分類される．


\subsubsection{自己符号化器の構成要素とそれぞれの役割}
Auto Encoderには大きく分けて，エンコーダー部分とデコーダー部分に分かれる．
エンコーダー部分でその特徴量を学習しながらその次元を圧縮し，それを基にデコーダー部分では復元を行う．
そのイメージを以下に示す．
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width = 6cm]{AE_image.png}
  \end{center}
\end{figure}


\subsubsection{主な応用先}
自己符号化器は次元削減や情報検索の分野で応用されてきた．\\
次元削減では自己符号化器で学習したパラメータを深層学習の初期パラメータとして使うことで
多次元のデータも効率よく学習ができ，次元を圧縮してもカテゴリ分類がうまくされていた．
低次元表現は分類問題で効果を発揮し，より効率よくタスクを分類できる．
また消費するメモリーと実行時間も短く出来る．
\\
情報検索とは入力データから似た項目を検索するタスクである．
入力から低次元のバイナリーを作成することで検索を効率化することが出来る．



\subsubsection{学習を成功させるための条件}
今回は実験を成功させるためにミニバッチ法をもちいた．
ミニバッチ法とは各パラメータの補正をある程度データがたまったときに行う手法である．

\subsubsection{主成分分析との関連}
主成分分析(Principle Component Analysis)とは，
学習データ
の分散が最大になる方向への線形変換を求める手法である．




\subsection{雑音除去自己符号化器（Denoising Autoencoder，DAE）}

雑音除去自己符号化器は破損したデータ点を入力として受け取りもとの破損していないデータ点を復元するよう訓練された自己複合化器である．
以下にDAEの訓練手順を示す．
\begin{enumerate}
  \item 入力データに一部ノイズを加えたデータをAEを行う
  \item 生成したデータとノイズをつける前のデータの誤差で逆伝播を行う
  \item 繰り返すことでノイズを除去するよう訓練していく
\end{enumerate}

このようにして学習した自己符号化器はノイズを除去するように復元するのでノイズを含んだ学習データから元の綺麗な画像を作り出すことができる

\section{実験方法}

実験データとしてCIFAR10データセットを用い，以下の目的が達成されるような実験方法を検討し，その方法を説明せよ．

\begin{itemize}
\item 学習後の自己符号化器が入力画像を正しく生成できることを確認する．
\item ネットワーク構成の違いによる生成画像の品質の差を確認する．畳み込み層を用いる場合とそうでない場合を比較せよ．
\item 学習後の符号化器が入力画像の特徴を把握できていることを確認する．
学習後の符号化器のパラメータを画像認識ネットワークの前段の初期値にした場合と，そうしない場合の正解率を比較せよ．
\item 雑音除去自己符号化器が入力画像の雑音を除去できることを確認する．
\end{itemize}

以下に注意すること．
\begin{itemize}
\item 実験条件ごとに番号をつけ，確認したい項目および実験結果の再現に必要な条件（ネットワーク構成，目的関数，パラメータの初期化方法，パラメータの更新方法，学習回数，学習およびテストデータ数，入力に付加する雑音の比率など）を明記すること．
\item テストデータで学習しないようにすること．
\end{itemize}

今回実験にて確認したいことは以下の２つである。

\begin{itemize}
  \item 画像がきちんとデノイジングされるかどうか
  \item 自己符号化器で学習したパラメータを使うと画像認識にどのような効果があるか
\end{itemize}

この二点について実験を行い、その効果を確認する．
上記のことを確認するために実験は次のように行う．

学習条件
\begin{itemize}
  \item 学習データ：Cifer10
  \item 学習データ数：100枚
  \item 分類テストデータ数：1000枚
  \item 学習回数：500回
  \item パラメーターの初期化方法：ガウス分布でランダムに初期化
  \item パラメータの更新方法：Adamアルゴリズムで更新
  \item ノイズの入れ方：画像の3割から5割をランダムで画素値を0にしてノイズとする
\end{itemize}


今回，実験で利用したネットワーク構成は以下のとおり．\\\\
\begin{table}[bt]
\begin{center}
\caption{オートエンコーダーARAの構成}
\label{table:AE:ARA}
\begin{tabularx}{0.9\linewidth}{|l|l|X|}
\hline
1 & アフィン層 & 入力チャネル数:$3$，入力サイズ:$32 \times 32$，出力チャネル数:$1$，出力サイズ:$500 $ \\
\hline
2 & 検出層 & ReLU \\
\hline
3 & Affine層 & 入力ノード数:$500$，出力チャネル数:$3$, 出力ノード数:$ 32 \times 32 $ \\
\hline
\end{tabularx}
\end{center}
\end{table}



\begin{table}[bt]
\begin{center}
\caption{オートエンコーダーcrpcrpaAの構成例}
\label{table:CRPCRPAA}
\begin{tabularx}{0.9\linewidth}{|l|l|X|}
\hline
1 & 畳み込み層 & 入力チャネル数:$3$，入力サイズ:$32 \times 32$，出力チャネル数:$10$，出力サイズ:$28 \times 28$，カーネルサイズ:$5 \times 5$ \\
\hline
2 & 検出層 & ReLU \\
\hline
3 & プーリング層 & 入力サイズ:$28 \times 28$，出力サイズ:$14 \times 14$，ブロックサイズ:$2 \times 2$，集約演算:最大値 \\
\hline
4 & 畳み込み層 & 入力チャネル数:$10$，入力サイズ:$14 \times 14$，出力チャネル数:$10$，出力サイズ:$10 \times 10$，カーネルサイズ:$5 \times 5$ \\
\hline
5 & 検出層 & ReLU \\
\hline
6 & プーリング層 & 入力サイズ:$10 \times 10$，出力サイズ:$5 \times 5$，ブロックサイズ:$2 \times 2$，集約演算:最大値 \\
\hline
7 & Affine層 & 入力ノード数:$250$，出力ノード数:$500$ \\
\hline
8 & Affine層 & 入力ノード数:$500$，出力チャネル数:$3$,出力ノード数:$ 32 \times 32 $ \\
\hline
\end{tabularx}
\end{center}
\end{table}



\clearpage

\section{実験結果}

以下のような結果が得られた．

\begin{figure}[h]
  \begin{center}
    \begin{tabular}{c}

      % 1
      \begin{minipage}{0.33\hsize}
        \begin{center}
          \includegraphics[clip, width=\linewidth]{cyfer_ans.jpg}
          \hspace{0.2cm} 学習データ
        \end{center}
      \end{minipage}

      % 2
      \begin{minipage}{0.33\hsize}
        \begin{center}
          \includegraphics[clip, width=\linewidth]{train_ln499_mode_ARA_ln_500-dn_100_tn_100_noise_0.png}
          \hspace{0.2cm} ARA:ノイズなし
        \end{center}
      \end{minipage}

      % 3
      \begin{minipage}{0.33\hsize}
        \begin{center}
          \includegraphics[clip, width=\linewidth]{train_ln250_mode_ARA_ln_300-dn_100_tn_100_noise_1.png}
          \hspace{0.2cm} ARA:ノイズあり
        \end{center}
      \end{minipage}


    \end{tabular}
    \caption{生成画像}
     \label{fig:accuracy}
  \end{center}
\end{figure}


\begin{figure}[h]
  \begin{center}
    \begin{tabular}{c}

      % 1
      \begin{minipage}{0.33\hsize}
        \begin{center}
          \includegraphics[clip, width=\linewidth]{cyfer_ans.jpg}
          \hspace{0.2cm} 学習データ
        \end{center}
      \end{minipage}

      % 2
      \begin{minipage}{0.33\hsize}
        \begin{center}
          \includegraphics[clip, width=\linewidth]{train_ln499_mode_crpcrpaA_ln_500-dn_100_tn_100_noise_0.png}
          \hspace{0.2cm} crpcrpaA:ノイズなし
        \end{center}
      \end{minipage}

      % 3
      \begin{minipage}{0.33\hsize}
        \begin{center}
          \includegraphics[clip, width=\linewidth]{train_ln250_mode_crpcrpaA_ln_300-dn_100_tn_100_noise_1.png}
          \hspace{0.2cm} crpcrpaA:ノイズあり
        \end{center}
      \end{minipage}


    \end{tabular}
    \caption{生成画像}
     \label{fig:accuracy}
  \end{center}
\end{figure}


\clearpage
\begin{figure}[h]
  \begin{center}
    \begin{tabular}{c}

      % 1
      \begin{minipage}{0.5\hsize}
        \begin{center}
          \includegraphics[clip, width=\linewidth]{MSE_ARA_ノイズなし.png}
          \hspace{0.2cm} ARA:平均二乗誤差
        \end{center}
      \end{minipage}

      % 2
      \begin{minipage}{0.5\hsize}
        \begin{center}
          \includegraphics[clip, width=\linewidth]{MSE_crpcrpaA_noise_0.png}
          \hspace{0.2cm} CRPCRPAA:平均二乗誤差
        \end{center}
      \end{minipage}

    \end{tabular}
    \caption{平均二乗誤差}

  \end{center}
\end{figure}


\begin{figure}[h]
  \begin{center}
    \begin{tabular}{c}

      % 1
      \begin{minipage}{0.5\hsize}
        \begin{center}
          \includegraphics[clip, width=\linewidth]{AR_ARA_noise_1.png}
          \hspace{0.2cm} ARA:正解率
        \end{center}
      \end{minipage}
      % 2
      \begin{minipage}{0.5\hsize}
        \begin{center}
          \includegraphics[clip, width=\linewidth]{AR_crpcrpaA_noise_0.png}
          \hspace{0.2cm} CRPCRPAA：正解率
        \end{center}
      \end{minipage}

    \end{tabular}
    \caption{正解率}

  \end{center}
\end{figure}




\clearpage


\section{考察}

今回、自己符号化器を作成し、それを用いた画像識別を行った。
さまざまな構成で試したが、ARAが最も良い結果を生んだ。
ARA、CRPCRPAともに学習回数を重ねるとノイズなしでは高い再現性を実現した。しかしながらその学習にはとても時間がかかる。
また自己符号化器を使った事前学習での画像識別は前回やったMNISTデータで学習する時よりも初期の正解率が高く、
平均二乗誤差も低い数値から始まり、結果として早く収束する傾向があるように思う．
これは自己符号化で早くからパラメータの数値が良い値を示し、収束の加速につながったと思う．
しかし同じ構成のネットワークで学習をしてみるとノイズなしの方がより良い結果を示しているように思う．
これはノイズをつけた方がノイズにフィットするように学習が進むためだと思われる．
\\
デノイジングに関してはこちらも学習回数というよりネットワーク構成が大きく影響することがわかる．
雑音除去はないものを周りのものを使って構成するという機能なのでかけてしまった歴史的な壁画や、
これを文章に置き換え穴あき問題を解かせるなどに使えそうだなと作成しながら考えていた。

今回はエンコーダーには畳み込み層を用いているが、デコーダ部分には畳み込みを持ちいらずアフィン層で復号した
この構成が結果にどのような影響をあたえているかわからないのでdeconvolutionを用いた実装をしてみたいと思う．

コーディングしていく中でうまくネットワークを作るにはトライアンドエラーが必要不可欠だと感じた．
おそらく元とする学習データでもその構成は変わると思われる．
なのでコツは改変しやすいようクラスやオブジェクトで各モジュールをパッケージ化していくことだと思われる．
より柔軟性の高いコーディングを目指していきたい．




\end{document}
